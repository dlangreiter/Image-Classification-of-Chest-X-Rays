{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Setup code**"
      ],
      "metadata": {
        "id": "TvhexBJdy5T2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2zD9rAhEWl_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import keras.backend as K\n",
        "\n",
        "# import numpy to create arrays for trial runs\n",
        "import numpy as np\n",
        "\n",
        "# matplotlib for visualisations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# if using Colab, for later saving of models and loading data\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "e4_LSbyfzK3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjcp1N61zGFo",
        "outputId": "6743229a-2e5d-47b5-f0d3-d77cc17d0265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-Processing**\n"
      ],
      "metadata": {
        "id": "Fui9KIqt2LKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BasePath = '/content/gdrive/MyDrive/BMET5933/Assignment2/chest_xray'\n",
        "\n",
        "TrainPath = '/content/gdrive/MyDrive/BMET5933/Assignment2/chest_xray/train'\n",
        "TestPath = '/content/gdrive/MyDrive/BMET5933/Assignment2/chest_xray/test'\n",
        "\n",
        "TrainPathNormal = '/content/gdrive/MyDrive/BMET5933/Assignment2/chest_xray/train/NORMAL'\n",
        "TrainPathPneu = '/content/gdrive/MyDrive/BMET5933/Assignment2/chest_xray/train/PNEUMONIA'\n",
        "\n",
        "TestPathNormal = '/content/gdrive/MyDrive/BMET5933/Assignment2/chest_xray/test/NORMAL'\n",
        "TestPathPneu = '/content/gdrive/MyDrive/BMET5933/Assignment2/chest_xray/test/PNEUMONIA'\n",
        "\n",
        "#2 ways to do it\n",
        "#- split pne into bacteria and viral\n",
        "#- you have 3 classes and have to compare against normal\n",
        "\n",
        "\n",
        "#- or group all togheter and comapre normal against pneuo"
      ],
      "metadata": {
        "id": "DxjFDYdW2Kq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import glob\n",
        "import pathlib\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import imghdr\n",
        "\n",
        "def image_dataset_from_dict(data_dict, image_size=(224, 224), batch_size=32, shuffle=True):\n",
        "    filenames = list(data_dict.keys())\n",
        "    labels = list(data_dict.values())\n",
        "    num_samples = len(filenames)\n",
        "\n",
        "    # Create a dataset from the filenames and labels\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "\n",
        "    # Map function to load and preprocess the images\n",
        "    def load_and_preprocess_image(filename, label):\n",
        "        image = tf.io.read_file(filename)\n",
        "        image = tf.image.decode_jpeg(image, channels=3)\n",
        "        image = tf.image.resize(image, image_size)\n",
        "        image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n",
        "        return image, label\n",
        "\n",
        "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=num_samples)\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Your code to create the dictionaries\n",
        "TrainBacterial = [filename for filename in os.listdir(TrainPathPneu) if filename.startswith(\"BACTERIA-\")]\n",
        "TestBacterial = [filename for filename in os.listdir(TestPathPneu) if filename.startswith(\"BACTERIA-\")]\n",
        "TrainViral = [filename for filename in os.listdir(TrainPathPneu) if filename.startswith(\"VIRUS-\")]\n",
        "TestViral = [filename for filename in os.listdir(TestPathPneu) if filename.startswith(\"VIRUS-\")]\n",
        "TrainNormal = [f for f in listdir(TrainPathNormal) if isfile(join(TrainPathNormal, f))]\n",
        "TestNormal =  [f for f in listdir(TestPathNormal) if isfile(join(TestPathNormal, f))]\n",
        "\n",
        "Train_labels = {}\n",
        "Test_labels = {}\n",
        "\n",
        "for image in TrainBacterial:\n",
        "    if imghdr.what(os.path.join(TrainPathPneu, image)) in [\"jpeg\", \"png\", \"gif\", \"bmp\"]:\n",
        "        Train_labels[os.path.join(TrainPathPneu, image)] = 0\n",
        "\n",
        "for image in TrainViral:\n",
        "    if imghdr.what(os.path.join(TrainPathPneu, image)) in [\"jpeg\", \"png\", \"gif\", \"bmp\"]:\n",
        "        Train_labels[os.path.join(TrainPathPneu, image)] = 1\n",
        "\n",
        "for image in TrainNormal:\n",
        "    if imghdr.what(os.path.join(TrainPathNormal, image)) in [\"jpeg\", \"png\", \"gif\", \"bmp\"]:\n",
        "        Train_labels[os.path.join(TrainPathNormal, image)] = 2\n",
        "\n",
        "for image in TestBacterial:\n",
        "    if imghdr.what(os.path.join(TestPathPneu, image)) in [\"jpeg\", \"png\", \"gif\", \"bmp\"]:\n",
        "        Test_labels[os.path.join(TestPathPneu, image)] = 0\n",
        "\n",
        "for image in TestViral:\n",
        "    if imghdr.what(os.path.join(TestPathPneu, image)) in [\"jpeg\", \"png\", \"gif\", \"bmp\"]:\n",
        "        Test_labels[os.path.join(TestPathPneu, image)] = 1\n",
        "\n",
        "for image in TestNormal:\n",
        "    if imghdr.what(os.path.join(TestPathNormal, image)) in [\"jpeg\", \"png\", \"gif\", \"bmp\"]:\n",
        "        Test_labels[os.path.join(TestPathNormal, image)] = 2\n",
        "\n",
        "# Create the dataset using the function and dictionaries\n",
        "train_dataset = image_dataset_from_dict(Train_labels, image_size=(224, 224), batch_size=32, shuffle=True)\n",
        "validation_dataset = image_dataset_from_dict(Test_labels, image_size=(224, 224), batch_size=32, shuffle=True)\n",
        "\n",
        "print(train_dataset)"
      ],
      "metadata": {
        "id": "jXkXvCBV9bDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))\n",
        "print(len(validation_dataset))"
      ],
      "metadata": {
        "id": "JOlnZh0tsKNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Deeper Model**"
      ],
      "metadata": {
        "id": "tFyjoQ7y7eKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import glob\n",
        "import pathlib\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import imghdr\n",
        "from scipy.ndimage import median_filter\n",
        "\n",
        "def image_dataset_from_dict(data_dict, image_size=(224, 224), batch_size=32, shuffle=True):\n",
        "    filenames = list(data_dict.keys())\n",
        "    labels = list(data_dict.values())\n",
        "    num_samples = len(filenames)\n",
        "\n",
        "    # Create a dataset from the filenames and labels\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "\n",
        "    # Map function to load and preprocess the images\n",
        "    def load_and_preprocess_image(filename, label):\n",
        "        image = tf.io.read_file(filename)\n",
        "        image = tf.image.decode_jpeg(image, channels=3)\n",
        "        image = tf.cast(image, tf.float32) / 255.0  # Convert to float32 and normalize to [0, 1]\n",
        "\n",
        "        # Apply histogram equalization\n",
        "        image = histogram_equalization(image)\n",
        "\n",
        "        # Apply median filtering\n",
        "        image = median_filtering(image)\n",
        "\n",
        "        image.set_shape((None, None, 3))  # Set the shape explicitly\n",
        "\n",
        "        image = tf.image.resize(image, image_size)\n",
        "        return image, label\n",
        "\n",
        "    def histogram_equalization(image):\n",
        "        # Convert image to HSV color space\n",
        "        image_hsv = tf.image.rgb_to_hsv(image)\n",
        "\n",
        "        # Extract the V channel\n",
        "        image_v = image_hsv[..., 2]\n",
        "\n",
        "        # Expand dimensions to match the expected rank\n",
        "        image_v = tf.expand_dims(image_v, axis=-1)\n",
        "\n",
        "        # Apply histogram equalization\n",
        "        image_v = tf.image.adjust_contrast(image_v, contrast_factor=2.0)\n",
        "\n",
        "        # Remove the extra dimensions\n",
        "        image_v = tf.squeeze(image_v, axis=-1)\n",
        "\n",
        "        # Combine the modified V channel with the original H and S channels\n",
        "        image_hsv_equalized = tf.stack([image_hsv[..., 0], image_hsv[..., 1], image_v], axis=-1)\n",
        "\n",
        "        # Convert the image back to RGB color space\n",
        "        image_equalized = tf.image.hsv_to_rgb(image_hsv_equalized)\n",
        "\n",
        "        return image_equalized\n",
        "\n",
        "    def median_filtering(image):\n",
        "        # Apply median filtering with a kernel size of 3x3\n",
        "        image_filtered = tf.numpy_function(median_filter, [image, (3, 3)], tf.float32)\n",
        "        return image_filtered\n",
        "\n",
        "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=num_samples)\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Your code to create the dictionaries\n",
        "TrainBacterial = [filename for filename in os.listdir(TrainPathPneu) if filename.startswith(\"BACTERIA-\")]\n",
        "TestBacterial = [filename for filename in os.listdir(TestPathPneu) if filename.startswith(\"BACTERIA-\")]\n",
        "TrainViral = [filename for filename in os.listdir(TrainPathPneu) if filename.startswith(\"VIRUS-\")]\n",
        "TestViral = [filename for filename in os.listdir(TestPathPneu) if filename.startswith(\"VIRUS-\")]\n",
        "TrainNormal = [f for f in listdir(TrainPathNormal) if isfile(join(TrainPathNormal, f))]\n",
        "TestNormal =  [f for f in listdir(TestPathNormal) if isfile(join(TestPathNormal, f))]\n",
        "\n",
        "Train_labels = {}\n",
        "Test_labels = {}\n",
        "\n",
        "for image in TrainBacterial:\n",
        "    if imghdr.what(os.path.join(TrainPathPneu, image)) in [\"jpeg\", \"png\", \"gif\", \"bmp\"]:\n",
        "        Train_labels[os.path.join(TrainPathPneu, image)] = 0\n",
        "\n",
        "for image in TrainViral:\n",
        "    if imghdr.what(os.path.join(TrainPathPneu, image)) in [\"jpeg\", \"png\", \"gif\", \"bmp\"]:\n",
        "        Train_labels[os.path.join(TrainPathPneu, image)] = 1\n",
        "\n",
        "for image in TrainNormal:\n",
        "    if imghdr.what(os.path.join(TrainPathNormal, image)) in [\"jpeg\", \"png\", \"gif\", \"bmp\"]:\n",
        "        Train_labels[os.path.join(TrainPathNormal, image)] = 2\n",
        "\n",
        "for image in TestBacterial:\n",
        "    if imghdr.what(os.path.join(TestPathPneu, image)) in [\"jpeg\", \"png\", \"gif\", \"bmp\"]:\n",
        "        Test_labels[os.path.join(TestPathPneu, image)] = 0\n",
        "\n",
        "for image in TestViral:\n",
        "    if imghdr.what(os.path.join(TestPathPneu, image)) in [\"jpeg\", \"png\", \"gif\", \"bmp\"]:\n",
        "        Test_labels[os.path.join(TestPathPneu, image)] = 1\n",
        "\n",
        "for image in TestNormal:\n",
        "    if imghdr.what(os.path.join(TestPathNormal, image)) in [\"jpeg\", \"png\", \"gif\", \"bmp\"]:\n",
        "        Test_labels[os.path.join(TestPathNormal, image)] = 2\n",
        "\n",
        "# Create the dataset using the function and dictionaries\n",
        "train_dataset = image_dataset_from_dict(Train_labels, image_size=(224, 224), batch_size=32, shuffle=True)\n",
        "validation_dataset = image_dataset_from_dict(Test_labels, image_size=(224, 224), batch_size=32, shuffle=True)\n",
        "\n",
        "print(train_dataset)"
      ],
      "metadata": {
        "id": "UGfRtDQb28ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "def create_model(input_shape, num_classes):\n",
        "    # Define the base model\n",
        "    base_model = keras.applications.ResNet50(\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\",\n",
        "        input_shape=input_shape,\n",
        "        pooling=\"avg\"\n",
        "    )\n",
        "\n",
        "    # Freeze the base model's weights\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Data augmentation\n",
        "    data_augmentation = keras.Sequential([\n",
        "        keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "        keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
        "    ])\n",
        "\n",
        "    # Add a classification head\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = data_augmentation(inputs)\n",
        "    x = base_model(x, training=False)\n",
        "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the input shape and number of classes\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 3\n",
        "\n",
        "# Create the model\n",
        "model = create_model(input_shape, num_classes)\n",
        "\n",
        "# Train the model using the dataset\n",
        "model.fit(train_dataset, validation_data=validation_dataset, epochs=10)"
      ],
      "metadata": {
        "id": "ZT_DVABXvwVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "def create_model(input_shape, num_classes):\n",
        "    # Define the base model\n",
        "    base_model = keras.applications.ResNet50(\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\",\n",
        "        input_shape=input_shape,\n",
        "        pooling=\"avg\"\n",
        "    )\n",
        "\n",
        "    # Set the last several layers to be trainable\n",
        "    num_layers_to_freeze = 150\n",
        "    for layer in base_model.layers[:-num_layers_to_freeze]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Data augmentation\n",
        "    data_augmentation = keras.Sequential([\n",
        "        keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "        keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
        "    ])\n",
        "\n",
        "    # Add a classification head\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = data_augmentation(inputs)\n",
        "    x = base_model(x, training=False)\n",
        "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the input shape and number of classes\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 3\n",
        "\n",
        "# Create the model\n",
        "model = create_model(input_shape, num_classes)\n",
        "\n",
        "# Train the model using the dataset with fine-tuning\n",
        "model.fit(train_dataset, validation_data=validation_dataset, epochs=10)"
      ],
      "metadata": {
        "id": "2lskJMqFyblB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_test_deeper(input_shape, num_classes):\n",
        "  # specify the size of the input\n",
        "  inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "  # recale from 0-255 to 0-1\n",
        "  x = layers.experimental.preprocessing.Rescaling(1.0/255) (inputs)\n",
        "  x = layers.Conv2D(32, (3,3), strides=1, padding=\"same\") (x)\n",
        "  x = layers.Activation(\"relu\") (x)\n",
        "  x = layers.Conv2D(32, (3,3), strides=1, padding=\"same\") (x)\n",
        "  x = layers.Activation(\"relu\") (x)\n",
        "  x = layers.MaxPooling2D(pool_size=3) (x)\n",
        "  x = layers.Conv2D(64, (3,3), strides=1, padding=\"same\") (x)\n",
        "  x = layers.Activation(\"relu\") (x)\n",
        "  x = layers.Conv2D(64, (3,3), strides=1, padding=\"same\") (x)\n",
        "  x = layers.Activation(\"relu\") (x)\n",
        "  x = layers.MaxPooling2D(pool_size=3) (x)\n",
        "  x = layers.Flatten() (x)\n",
        "\n",
        "  # OFTEN NEED A FLATTEN AS THE INPUT TO DENSE\n",
        "  # output is a Dense with softmax\n",
        "  outputs = layers.Dense(num_classes, activation=\"softmax\") (x)\n",
        "\n",
        "  # return the model\n",
        "  return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "NUM_CLASSES=3\n",
        "image_size = (224, 224, 3)\n",
        "model_test_deeper = make_model_test_deeper(input_shape=image_size, num_classes=NUM_CLASSES)\n",
        "\n",
        "\n",
        "# set some hyperparameters\n",
        "LEARNING_RATE = 0.0001\n",
        "BATCH_SIZE = 32 # should be the same as the dataset batch size\n",
        "NUM_EPOCHS = 10  # how many epochs to train for (each epoch visits the training data once)\n",
        "\n",
        "# compile the model\n",
        "model_test_deeper.compile(\n",
        "    optimizer=keras.optimizers.Adam(LEARNING_RATE),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "train_history = model_test_deeper.fit(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    validation_data=validation_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "sA1tHZde7dpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = train_history.history['accuracy']\n",
        "val_acc = train_history.history['val_accuracy']\n",
        "\n",
        "loss = train_history.history['loss']\n",
        "val_loss = train_history.history['val_loss']\n",
        "\n",
        "epochs_range = range(NUM_EPOCHS)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0KlHosXnqCxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E3Zh1QRH8HMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(validation_dataset)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "cr0vH1S4_R3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://link-springer-com.ezproxy.library.sydney.edu.au/article/10.1007/s00521-023-08450-y\n",
        "\n",
        "This procedure requires the application of the MakeSense data labelling tool. MakeSense is a free online application for image labelling. It draws polygons to completely outline images’ objects. For this study, labels are saved as “.json” files for mask R-CNN, and the “.vgg” format for faster R-CNN. In this study, a total of 4000 images, including 3200 training images and 800 validation images, are labelled."
      ],
      "metadata": {
        "id": "QTaGAJqFBNJ6"
      }
    }
  ]
}